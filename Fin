To build an automated pipeline that handles input from 100 cameras, converts and streams them live on a website, using FFmpeg, NGINX, and a load balancer, follow this step-by-step architecture:


---

1. System Architecture Overview

Components:

Cameras (RTSP/RTMP/etc. sources)

FFmpeg (for ingest and convert streams)

NGINX with RTMP module (media server)

Load balancer (NGINX, HAProxy, or cloud-based)

Web server with frontend UI (React, Vue, etc.)

Service orchestrator (e.g., systemd, Docker, Kubernetes)



---

2. Automatic FFmpeg Streaming Pipeline

Step 1: Camera Detection & Triggering FFmpeg

Use a daemon or event-based script to monitor for camera online status.

Could be ping, heartbeat, or motion detection.


Automatically start an FFmpeg process per camera stream.


#!/bin/bash
CAMERA_URL="$1"
STREAM_KEY="$2"

ffmpeg -i "$CAMERA_URL" -c:v libx264 -f flv "rtmp://<nginx-server>/live/$STREAM_KEY"

This script can be run using:

Systemd services

Docker containers

Kubernetes pods (with HPA/autoscaling)




---

3. NGINX Configuration

Use NGINX with the RTMP module to receive and distribute streams.

nginx.conf (RTMP block)

rtmp {
    server {
        listen 1935;
        chunk_size 4096;

        application live {
            live on;
            record off;
        }
    }
}

You can also serve HLS/DASH with NGINX and convert streams via FFmpeg.


---

4. Load Balancer

For 100 cameras, a single NGINX may not scale. Use a load balancer to:

Distribute traffic across multiple NGINX-RTMP instances.

Use DNS round-robin, HAProxy, or NGINX upstream config.


Example NGINX LB Config:

upstream media_servers {
    server stream1.example.com;
    server stream2.example.com;
}

server {
    listen 80;

    location /live/ {
        proxy_pass http://media_servers;
    }
}


---

5. Frontend (Web Page)

Use JavaScript + <video> or libraries like Video.js to show live streams.

<video id="cam1" controls autoplay>
  <source src="http://example.com/hls/cam1.m3u8" type="application/x-mpegURL">
</video>

Each stream can be accessed via /live/{stream_key}.m3u8



---

6. Automation Layer

Use Kubernetes or Docker Compose to auto-scale FFmpeg services and media servers.

Option A: Docker Compose (Simple)

Define a service for each camera using a template.


Option B: Kubernetes (Scalable)

Use Helm charts or custom operators to spin up FFmpeg jobs dynamically based on camera input.

Add readiness/liveness probes.

Autoscale based on CPU/network load.



---

7. Health Monitoring

Use Prometheus + Grafana to monitor FFmpeg and NGINX.

Implement alerts if streams go down.



---

8. Security

Token-based authentication on stream keys.

NGINX IP whitelisting or auth middleware.

Use HTTPS for frontend.



---

Would you like a working example in Docker Compose or Kubernetes YAML to bootstrap this setup?

Yes, you can absolutely automate the pipeline without Docker or Kubernetes by using traditional system services and scripting. Here’s how to build the automated FFmpeg + NGINX pipeline for 100 cameras using only bash, systemd, and NGINX, without Docker or Kubernetes:


---

Step-by-Step Automated Pipeline (No Docker/K8s)

1. Define Camera List

Maintain a central config file like /etc/camera_list.txt:

cam01 rtsp://192.168.1.10/stream
cam02 rtsp://192.168.1.11/stream
...
cam100 rtsp://192.168.1.110/stream


---

2. Write a Shell Script to Start FFmpeg for Each Camera

Create /usr/local/bin/start_camera_streams.sh:

#!/bin/bash

CAMERA_LIST="/etc/camera_list.txt"
NGINX_STREAM_URL="rtmp://localhost/live"

while IFS=" " read -r CAM_NAME CAM_URL
do
    echo "Starting stream for $CAM_NAME"

    ffmpeg -rtsp_transport tcp -i "$CAM_URL" \
        -c:v libx264 -preset ultrafast -tune zerolatency \
        -f flv "$NGINX_STREAM_URL/$CAM_NAME" \
        > "/var/log/ffmpeg_$CAM_NAME.log" 2>&1 &

    echo $! > "/var/run/ffmpeg_$CAM_NAME.pid"
done < "$CAMERA_LIST"

Each camera runs in the background with logs and PID stored.



---

3. Systemd Service to Auto-Start at Boot

Create a systemd unit file: /etc/systemd/system/streaming.service

[Unit]
Description=Start All Camera Streams
After=network.target

[Service]
ExecStart=/usr/local/bin/start_camera_streams.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target

Then:

sudo systemctl daemon-reexec
sudo systemctl enable --now streaming.service


---

4. Configure NGINX with RTMP Module

Ensure NGINX has the RTMP module. Example /etc/nginx/nginx.conf:

rtmp {
    server {
        listen 1935;
        chunk_size 4096;

        application live {
            live on;
            record off;
        }
    }
}

http {
    server {
        listen 8080;

        location /stat {
            rtmp_stat all;
            rtmp_stat_stylesheet stat.xsl;
        }

        location /stat.xsl {
            root /usr/local/nginx/html;
        }

        location /hls {
            types {
                application/vnd.apple.mpegurl m3u8;
                video/mp2t ts;
            }
            root /tmp;
            add_header Cache-Control no-cache;
        }
    }
}


---

5. Auto Reload on Camera Reconnects (Optional)

To detect camera restarts dynamically:

Use cron or watchdog scripts to ping cameras or use ffprobe to check stream health.

Restart individual FFmpeg processes when a stream dies.



---

6. Frontend Streaming (HTML Page)

Use basic HTML:

<video width="640" height="360" controls autoplay>
  <source src="http://yourdomain/hls/cam01.m3u8" type="application/x-mpegURL">
</video>

You’ll need to enable HLS output from FFmpeg, or generate HLS files for NGINX to serve.


---

Summary

Yes, you can automate everything without containers using:

Shell scripts to manage FFmpeg

Systemd for persistence and auto-start

NGINX with RTMP to broadcast

Frontend to view with <video> or Video.js

Log and PID files to manage and monitor jobs



---

Would you like me to generate the FFmpeg HLS command and sample HTML for the website as the next step?

Here’s how to generate HLS output from FFmpeg and stream it on a website using basic HTML.


---

Step 1: FFmpeg Command for HLS Output

Instead of pushing to NGINX RTMP, you can use FFmpeg to generate .m3u8 playlists and .ts segments directly on disk, which NGINX serves over HTTP.

Example FFmpeg command for camera:

#!/bin/bash

CAM_NAME="$1"
CAM_URL="$2"
HLS_DIR="/var/www/html/hls/$CAM_NAME"

mkdir -p "$HLS_DIR"

ffmpeg -rtsp_transport tcp -i "$CAM_URL" \
    -c:v libx264 -preset ultrafast -tune zerolatency -c:a aac -ar 44100 -ac 1 \
    -f hls \
    -hls_time 2 \
    -hls_list_size 5 \
    -hls_flags delete_segments \
    "$HLS_DIR/index.m3u8"

Output is: /var/www/html/hls/cam01/index.m3u8


Make this a looped script for 100 cameras as before.


---

Step 2: Configure NGINX to Serve HLS

Ensure your NGINX http block includes:

server {
    listen 80;
    server_name yourdomain.com;

    location /hls/ {
        types {
            application/vnd.apple.mpegurl m3u8;
            video/mp2t ts;
        }
        root /var/www/html;
        add_header Cache-Control no-cache;
    }
}

Reload NGINX:

sudo nginx -s reload


---

Step 3: HTML Page to Show the Stream

Basic HTML page:

<!DOCTYPE html>
<html>
<head>
  <title>Live Camera Stream</title>
</head>
<body>
  <h1>Camera 01</h1>
  <video width="640" height="360" controls autoplay muted>
    <source src="http://yourdomain.com/hls/cam01/index.m3u8" type="application/x-mpegURL">
    Your browser does not support the video tag.
  </video>
</body>
</html>

> Note: HLS is supported natively in Safari. For Chrome/Firefox, use Video.js or hls.js.




---

Optional Enhancements

Serve with HTTPS (required on some browsers for autoplay).

Use a template engine (Jinja2, PHP, etc.) to auto-generate the HTML for 100 cameras.

Use systemd or cron to restart FFmpeg if a stream dies.

Monitor with ps, ffprobe, or logging.



---

Let me know if you'd like:

A full bash script that spins up 100 HLS streams.

A sample dynamic HTML dashboard for all cameras.


